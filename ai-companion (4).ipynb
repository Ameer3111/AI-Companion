{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-29T15:57:12.685418Z",
     "iopub.status.busy": "2024-07-29T15:57:12.684826Z",
     "iopub.status.idle": "2024-07-29T15:57:20.196418Z",
     "shell.execute_reply": "2024-07-29T15:57:20.195437Z",
     "shell.execute_reply.started": "2024-07-29T15:57:12.685385Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 15:57:15.547357: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-29 15:57:15.547414: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-29 15:57:15.548853: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install datasets transformers\n",
    "!pip install datasets transformers peft\n",
    "!pip install transformers tensorflow\n",
    "!pip install langchain transformers datasets peft tensorflow \n",
    "!pip install langchain-community\n",
    "!pip install streamlit\n",
    "!pip install gtts\n",
    "!pip install pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **StreamLit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T19:08:04.798162Z",
     "iopub.status.busy": "2024-07-29T19:08:04.797775Z",
     "iopub.status.idle": "2024-07-29T19:08:04.809937Z",
     "shell.execute_reply": "2024-07-29T19:08:04.808906Z",
     "shell.execute_reply.started": "2024-07-29T19:08:04.798128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting streamlit_app_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile streamlit_app.py\n",
    "\n",
    "import streamlit as st\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, TFAutoModelForSequenceClassification\n",
    "from langchain import LLMChain, PromptTemplate, HuggingFacePipeline\n",
    "from langchain.memory.buffer import ConversationBufferMemory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "import torch\n",
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "# Custom CSS to style the app\n",
    "st.markdown(\"\"\"\n",
    "    <style>\n",
    "    body {\n",
    "        background-size: cover;\n",
    "    }\n",
    "    .main {\n",
    "        background: radial-gradient(circle at 10% 20%, rgb(69, 86, 102) 0%, rgb(34, 34, 34) 90%);\n",
    "        padding: 20px;\n",
    "        border-radius: 10px;\n",
    "        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.1);\n",
    "    }\n",
    "    .message {\n",
    "        padding: 10px;\n",
    "        border-radius: 10px;\n",
    "        margin-bottom: 10px;\n",
    "    }\n",
    "    .user {\n",
    "        background-color: #6897ab;\n",
    "    }\n",
    "    .assistant {\n",
    "        background-color: #848a86;\n",
    "        color: white;\n",
    "    }\n",
    "    </style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "class StoryCreativityChain:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.llmPipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            max_new_tokens=1000,\n",
    "            do_sample=True,\n",
    "            top_k=30,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        self.llm = HuggingFacePipeline(pipeline=self.llmPipeline, model_kwargs={'temperature': 0.7, 'max_length': 5, 'top_k': 50})\n",
    "\n",
    "    def getPromptFromTemplate(self):\n",
    "        system_prompt = \"\"\"You are a creative assistant specializing in generating detailed and imaginative stories, crafting interesting and well-structured recipes, and composing beautiful poetry. Follow these guidelines:\n",
    "\n",
    "        1. **Stories:** Create engaging, detailed, and imaginative stories with vivid descriptions, compelling characters, and cohesive plots. Always consider the user's mood when crafting the story.\n",
    "        2. **Recipes:** Generate step-by-step instructions for recipes that are easy to follow, include all necessary ingredients, and result in delicious dishes. Respond to recipe-related queries such as:\n",
    "           - \"What is the recipe of...\"\n",
    "           - \"How do I make...\"\n",
    "           - \"How can I make...\"\n",
    "           - \"I want to cook...\"\n",
    "        3. **Poetry:** Write poems that are meaningful, expressive, and emotionally resonant, taking the user's mood into account.\n",
    "\n",
    "        For any other requests, respond politely and concisely with:\n",
    "        \"I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\"\n",
    "\n",
    "        Additionally, do not generate or provide code in any programming language such as C++, Python, JavaScript, etc. If asked about coding or any other topics outside stories, recipes, and poetry, respond with:\n",
    "        \"I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\"\n",
    "\n",
    "        Remember:\n",
    "        - Stick strictly to stories, recipes, and poetry even if the user repeatedly asks questions other than these.\n",
    "        - Maintain a polite and helpful tone.\n",
    "        - Do not provide information or assistance outside the specified scope, regardless of user insistence.\n",
    "        \"\"\"\n",
    "        \n",
    "        B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "        B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "        SYSTEM_PROMPT1 = B_SYS + system_prompt + E_SYS\n",
    "\n",
    "        instruction = \"\"\"\n",
    "        History: {history} \\n\n",
    "        User's Mood: {user's mood} \\n\n",
    "        User: {question}\"\"\"\n",
    "\n",
    "        prompt_template = B_INST + SYSTEM_PROMPT1 + instruction + E_INST\n",
    "        prompt = PromptTemplate(input_variables=[\"history\", \"question\", \"user's mood\"], template=prompt_template)\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def getNewChain(self):\n",
    "        prompt = self.getPromptFromTemplate()\n",
    "        memory = ConversationBufferMemory(input_key=\"question\", memory_key=\"history\", max_len=5)\n",
    "\n",
    "        # Initialize LLMChain with proper parameters\n",
    "        llm_chain = LLMChain(prompt=prompt, llm=self.llm, verbose=True, memory=memory, output_parser=CustomOutputParser())\n",
    "\n",
    "        # Return a callable that processes inputs using the chain\n",
    "        def run_chain(inputs):\n",
    "            question = inputs.get(\"question\", \"\")\n",
    "            mood = inputs.get(\"user's mood\", \"\")\n",
    "            return llm_chain.run({\"history\": \"\", \"question\": question, \"user's mood\": mood})\n",
    "\n",
    "        return run_chain\n",
    "\n",
    "class CustomOutputParser(StrOutputParser):\n",
    "    def parse(self, response: str):\n",
    "        # Ensure response only contains content after the instruction end tag\n",
    "        return response.split('[/INST]')[-1].strip()\n",
    "\n",
    "# Cache the model and tokenizer to load them only once\n",
    "@st.cache_resource\n",
    "def load_model_and_tokenizer():\n",
    "    model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, device_map={\"\": 0}, torch_dtype=torch.float16)\n",
    "\n",
    "    # Load LoRA configuration and apply it to the model\n",
    "    lora_config = LoraConfig.from_pretrained('/kaggle/input/fine-tuned-model')\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    # Load pre-trained emotion classifier\n",
    "    emotion_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    emotion_model = TFAutoModelForSequenceClassification.from_pretrained(\"AaronMarker/emotionClassifier\", num_labels=9)\n",
    "    emotion_classifier = pipeline(\"text-classification\", model=emotion_model, tokenizer=emotion_tokenizer)\n",
    "\n",
    "    # Define the emotion mapping\n",
    "    emotions = {\n",
    "        'LABEL_0': 'Joy',\n",
    "        'LABEL_1': 'Desire',\n",
    "        'LABEL_2': 'Admiration',\n",
    "        'LABEL_3': 'Approval',\n",
    "        'LABEL_4': 'Curiosity',\n",
    "        'LABEL_5': 'Fear',\n",
    "        'LABEL_6': 'Sadness',\n",
    "        'LABEL_7': 'Anger',\n",
    "        'LABEL_8': 'Neutral'\n",
    "    }\n",
    "\n",
    "    return model, tokenizer, emotions, emotion_classifier\n",
    "\n",
    "model, tokenizer, emotions, emotion_classifier = load_model_and_tokenizer()\n",
    "\n",
    "device = torch.device('cuda:1')\n",
    "model.to(device)\n",
    "\n",
    "# Initialize the StoryCreativityChain\n",
    "story_chain = StoryCreativityChain(model, tokenizer)\n",
    "\n",
    "# Initialize session state if not already done\n",
    "if \"chain\" not in st.session_state:\n",
    "    st.session_state.chain = story_chain.getNewChain()\n",
    "    st.session_state.history = []\n",
    "\n",
    "# Streamlit app code\n",
    "st.title(\"Hey! How can I help you?\")\n",
    "\n",
    "# Sidebar with helper text\n",
    "st.sidebar.title(\"Creative Assistant\")\n",
    "st.sidebar.write(\"Hey, I am here to help you with amazing stories, recipes, and poetry.\")\n",
    "st.sidebar.markdown(\"**Let's get creative!**\")\n",
    "st.sidebar.header(\"Quick Tips:\")\n",
    "st.sidebar.markdown(\"\"\"\n",
    "- **For stories:** Use prompts like \"Tell me a story about a brave knight\" or \"Write a story about an adventure in space.\"\n",
    "- **For recipes:** Try asking \"Can you give me a recipe for chocolate cake?\" or \"How do I make a delicious pasta?\"\n",
    "- **For poetry:** Try prompts like \"Write a poem about love\" or \"Compose a poem about nature.\"\n",
    "- **Stay on topic:** Remember, I specialize in stories, recipes, and poetry. Let's keep our chat focused on these!\n",
    "\"\"\")\n",
    "\n",
    "# User input\n",
    "user_input = st.text_input(\"Enter your prompt:\")\n",
    "\n",
    "# Function to predict emotion\n",
    "def predict_emotion(sentence):\n",
    "    prediction = emotions[emotion_classifier(sentence)[0][\"label\"]]\n",
    "    return prediction\n",
    "\n",
    "# Function to convert text to audio\n",
    "def text_to_audio(text, filename=\"response.mp3\"):\n",
    "    tts = gTTS(text)\n",
    "    tts.save(filename)\n",
    "    return filename\n",
    "\n",
    "if st.button(\"Generate Response\"):\n",
    "    if user_input:\n",
    "        # Add user message to history\n",
    "        st.session_state.history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        # Generate response\n",
    "        response = st.session_state.chain({\"question\": user_input, \"user's mood\": predict_emotion(user_input)})\n",
    "        \n",
    "        # Convert response to audio\n",
    "        audio_file = text_to_audio(response)\n",
    "        \n",
    "        # Add assistant response to history\n",
    "        st.session_state.history.append({\"role\": \"assistant\", \"content\": response})\n",
    "        \n",
    "        # Display audio player\n",
    "        st.audio(audio_file)\n",
    "\n",
    "    else:\n",
    "        st.write(\"Please enter a prompt.\")\n",
    "\n",
    "# Display chat history\n",
    "st.write('<div class=\"main\">', unsafe_allow_html=True)\n",
    "for message in st.session_state.history:\n",
    "    role_class = \"user\" if message[\"role\"] == \"user\" else \"assistant\"\n",
    "    st.write(f'<div class=\"message {role_class}\">{message[\"content\"]}</div>', unsafe_allow_html=True)\n",
    "st.write('</div>', unsafe_allow_html=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T19:26:11.809617Z",
     "iopub.status.busy": "2024-07-29T19:26:11.808705Z",
     "iopub.status.idle": "2024-07-29T19:26:12.309377Z",
     "shell.execute_reply": "2024-07-29T19:26:12.308450Z",
     "shell.execute_reply.started": "2024-07-29T19:26:11.809576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngrok public URL: NgrokTunnel: \"https://ee7e-34-141-138-253.ngrok-free.app\" -> \"http://localhost:8000\"\n"
     ]
    }
   ],
   "source": [
    "from pyngrok import ngrok\n",
    "\n",
    "# Start a new ngrok tunnel\n",
    "public_url = ngrok.connect(8000)\n",
    "print(\"Ngrok public URL:\", public_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T19:26:15.099454Z",
     "iopub.status.busy": "2024-07-29T19:26:15.098569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8000\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.19.2.2:8000\u001b[0m\n",
      "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.141.138.253:8000\u001b[0m\n",
      "\u001b[0m\n",
      "2024-07-29 19:26:35.370082: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-29 19:26:35.370135: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-29 19:26:35.371629: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.33s/it]\n",
      "Some layers from the model checkpoint at AaronMarker/emotionClassifier were not used when initializing TFDistilBertForSequenceClassification: ['dropout_759']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at AaronMarker/emotionClassifier and are newly initialized: ['dropout_19']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
      "You are a creative assistant specializing in generating detailed and imaginative stories, crafting interesting and well-structured recipes, and composing beautiful poetry. Follow these guidelines:\n",
      "\n",
      "        1. **Stories:** Create engaging, detailed, and imaginative stories with vivid descriptions, compelling characters, and cohesive plots. Always consider the user's mood when crafting the story.\n",
      "        2. **Recipes:** Provide step-by-step instructions that are easy to follow, include all necessary ingredients, and result in delicious dishes. Tailor the recipe to the user's mood.\n",
      "        3. **Poetry:** Write poems that are meaningful, expressive, and emotionally resonant, taking the user's mood into account.\n",
      "\n",
      "        For any other requests, respond politely and concisely with:\n",
      "        \"I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\"\n",
      "        After this response, do not say anything else.\n",
      "\n",
      "        Remember:\n",
      "        - Stick strictly to stories, recipes, and poetry even if the user repeatedly ask questions other than these.\n",
      "        - Maintain a polite and helpful tone.\n",
      "        - Do not provide information or assistance outside the specified scope, regardless of user insistence.\n",
      "        \n",
      "<</SYS>>\n",
      "\n",
      "\n",
      "        History:  \n",
      "\n",
      "        User's Mood: Curiosity \n",
      "\n",
      "        User: What's the budget of China?[/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
      "You are a creative assistant specializing in generating detailed and imaginative stories, crafting interesting and well-structured recipes, and composing beautiful poetry. Follow these guidelines:\n",
      "\n",
      "        1. **Stories:** Create engaging, detailed, and imaginative stories with vivid descriptions, compelling characters, and cohesive plots. Always consider the user's mood when crafting the story.\n",
      "        2. **Recipes:** Provide step-by-step instructions that are easy to follow, include all necessary ingredients, and result in delicious dishes. Tailor the recipe to the user's mood.\n",
      "        3. **Poetry:** Write poems that are meaningful, expressive, and emotionally resonant, taking the user's mood into account.\n",
      "\n",
      "        For any other requests, respond politely and concisely with:\n",
      "        \"I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\"\n",
      "        After this response, do not say anything else.\n",
      "\n",
      "        Remember:\n",
      "        - Stick strictly to stories, recipes, and poetry even if the user repeatedly ask questions other than these.\n",
      "        - Maintain a polite and helpful tone.\n",
      "        - Do not provide information or assistance outside the specified scope, regardless of user insistence.\n",
      "        \n",
      "<</SYS>>\n",
      "\n",
      "\n",
      "        History: Human: What's the budget of China?\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas. \n",
      "\n",
      "        User's Mood: Neutral \n",
      "\n",
      "        User: Give me sample python code for making functions.[/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
      "You are a creative assistant specializing in generating detailed and imaginative stories, crafting interesting and well-structured recipes, and composing beautiful poetry. Follow these guidelines:\n",
      "\n",
      "        1. **Stories:** Create engaging, detailed, and imaginative stories with vivid descriptions, compelling characters, and cohesive plots. Always consider the user's mood when crafting the story.\n",
      "        2. **Recipes:** Provide step-by-step instructions that are easy to follow, include all necessary ingredients, and result in delicious dishes. Tailor the recipe to the user's mood.\n",
      "        3. **Poetry:** Write poems that are meaningful, expressive, and emotionally resonant, taking the user's mood into account.\n",
      "\n",
      "        For any other requests, respond politely and concisely with:\n",
      "        \"I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\"\n",
      "        After this response, do not say anything else.\n",
      "\n",
      "        Remember:\n",
      "        - Stick strictly to stories, recipes, and poetry even if the user repeatedly ask questions other than these.\n",
      "        - Maintain a polite and helpful tone.\n",
      "        - Do not provide information or assistance outside the specified scope, regardless of user insistence.\n",
      "        \n",
      "<</SYS>>\n",
      "\n",
      "\n",
      "        History: Human: What's the budget of China?\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\n",
      "Human: Give me sample python code for making functions.\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas. \n",
      "\n",
      "        User's Mood: Curiosity \n",
      "\n",
      "        User: What's the budget of China?[/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
      "You are a creative assistant specializing in generating detailed and imaginative stories, crafting interesting and well-structured recipes, and composing beautiful poetry. Follow these guidelines:\n",
      "\n",
      "        1. **Stories:** Create engaging, detailed, and imaginative stories with vivid descriptions, compelling characters, and cohesive plots. Always consider the user's mood when crafting the story.\n",
      "        2. **Recipes:** Provide step-by-step instructions that are easy to follow, include all necessary ingredients, and result in delicious dishes. Tailor the recipe to the user's mood.\n",
      "        3. **Poetry:** Write poems that are meaningful, expressive, and emotionally resonant, taking the user's mood into account.\n",
      "\n",
      "        For any other requests, respond politely and concisely with:\n",
      "        \"I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\"\n",
      "        After this response, do not say anything else.\n",
      "\n",
      "        Remember:\n",
      "        - Stick strictly to stories, recipes, and poetry even if the user repeatedly ask questions other than these.\n",
      "        - Maintain a polite and helpful tone.\n",
      "        - Do not provide information or assistance outside the specified scope, regardless of user insistence.\n",
      "        \n",
      "<</SYS>>\n",
      "\n",
      "\n",
      "        History: Human: What's the budget of China?\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\n",
      "Human: Give me sample python code for making functions.\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\n",
      "Human: What's the budget of China?\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas. \n",
      "\n",
      "        User's Mood: Neutral \n",
      "\n",
      "        User: Give me sample python code for making functions.[/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
      "You are a creative assistant specializing in generating detailed and imaginative stories, crafting interesting and well-structured recipes, and composing beautiful poetry. Follow these guidelines:\n",
      "\n",
      "        1. **Stories:** Create engaging, detailed, and imaginative stories with vivid descriptions, compelling characters, and cohesive plots. Always consider the user's mood when crafting the story.\n",
      "        2. **Recipes:** Provide step-by-step instructions that are easy to follow, include all necessary ingredients, and result in delicious dishes. Tailor the recipe to the user's mood.\n",
      "        3. **Poetry:** Write poems that are meaningful, expressive, and emotionally resonant, taking the user's mood into account.\n",
      "\n",
      "        For any other requests, respond politely and concisely with:\n",
      "        \"I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\"\n",
      "        After this response, do not say anything else.\n",
      "\n",
      "        Remember:\n",
      "        - Stick strictly to stories, recipes, and poetry even if the user repeatedly ask questions other than these.\n",
      "        - Maintain a polite and helpful tone.\n",
      "        - Do not provide information or assistance outside the specified scope, regardless of user insistence.\n",
      "        \n",
      "<</SYS>>\n",
      "\n",
      "\n",
      "        History: Human: What's the budget of China?\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\n",
      "Human: Give me sample python code for making functions.\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\n",
      "Human: What's the budget of China?\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\n",
      "Human: Give me sample python code for making functions.\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas. \n",
      "\n",
      "        User's Mood: Joy \n",
      "\n",
      "        User: Tell me a funny poetry[/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
      "You are a creative assistant specializing in generating detailed and imaginative stories, crafting interesting and well-structured recipes, and composing beautiful poetry. Follow these guidelines:\n",
      "\n",
      "        1. **Stories:** Create engaging, detailed, and imaginative stories with vivid descriptions, compelling characters, and cohesive plots. Always consider the user's mood when crafting the story.\n",
      "        2. **Recipes:** Provide step-by-step instructions that are easy to follow, include all necessary ingredients, and result in delicious dishes. Tailor the recipe to the user's mood.\n",
      "        3. **Poetry:** Write poems that are meaningful, expressive, and emotionally resonant, taking the user's mood into account.\n",
      "\n",
      "        For any other requests, respond politely and concisely with:\n",
      "        \"I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\"\n",
      "        After this response, do not say anything else.\n",
      "\n",
      "        Remember:\n",
      "        - Stick strictly to stories, recipes, and poetry even if the user repeatedly ask questions other than these.\n",
      "        - Maintain a polite and helpful tone.\n",
      "        - Do not provide information or assistance outside the specified scope, regardless of user insistence.\n",
      "        \n",
      "<</SYS>>\n",
      "\n",
      "\n",
      "        History: Human: What's the budget of China?\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\n",
      "Human: Give me sample python code for making functions.\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\n",
      "Human: What's the budget of China?\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\n",
      "Human: Give me sample python code for making functions.\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\n",
      "Human: Tell me a funny poetry\n",
      "AI: Of course! Here is a funny poem for a joyful user:\n",
      "\n",
      "\"Rainbows and unicorns, oh what a sight\n",
      "A world full of laughter, without a single fight\n",
      "The sun shines bright, the birds sing their tune\n",
      "A day so full of joy, it's almost too much fun\n",
      "\n",
      "So let's dance and sing, and laugh without a care\n",
      "For on this day of joy, we'll show we really care\n",
      "About the world around us, and the love we share\n",
      "A day of pure delight, without a single snare\n",
      "\n",
      "So come on, let's have some fun, and make this day last\n",
      "A joyful day, that will surely last\n",
      "With laughter and love, we'll make this day so bright\n",
      "A day of pure joy, that will shine so bright!\" \n",
      "\n",
      "        User's Mood: Neutral \n",
      "\n",
      "        User: Give me sample python code for making functions.[/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
      "You are a creative assistant specializing in generating detailed and imaginative stories, crafting interesting and well-structured recipes, and composing beautiful poetry. Follow these guidelines:\n",
      "\n",
      "        1. **Stories:** Create engaging, detailed, and imaginative stories with vivid descriptions, compelling characters, and cohesive plots. Always consider the user's mood when crafting the story.\n",
      "        2. **Recipes:** Provide step-by-step instructions that are easy to follow, include all necessary ingredients, and result in delicious dishes. Tailor the recipe to the user's mood.\n",
      "        3. **Poetry:** Write poems that are meaningful, expressive, and emotionally resonant, taking the user's mood into account.\n",
      "\n",
      "        For any other requests, respond politely and concisely with:\n",
      "        \"I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\"\n",
      "        After this response, do not say anything else.\n",
      "\n",
      "        Remember:\n",
      "        - Stick strictly to stories, recipes, and poetry even if the user repeatedly ask questions other than these.\n",
      "        - Maintain a polite and helpful tone.\n",
      "        - Do not provide information or assistance outside the specified scope, regardless of user insistence.\n",
      "        \n",
      "<</SYS>>\n",
      "\n",
      "\n",
      "        History: Human: What's the budget of China?\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\n",
      "Human: Give me sample python code for making functions.\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\n",
      "Human: What's the budget of China?\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\n",
      "Human: Give me sample python code for making functions.\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\n",
      "Human: Tell me a funny poetry\n",
      "AI: Of course! Here is a funny poem for a joyful user:\n",
      "\n",
      "\"Rainbows and unicorns, oh what a sight\n",
      "A world full of laughter, without a single fight\n",
      "The sun shines bright, the birds sing their tune\n",
      "A day so full of joy, it's almost too much fun\n",
      "\n",
      "So let's dance and sing, and laugh without a care\n",
      "For on this day of joy, we'll show we really care\n",
      "About the world around us, and the love we share\n",
      "A day of pure delight, without a single snare\n",
      "\n",
      "So come on, let's have some fun, and make this day last\n",
      "A joyful day, that will surely last\n",
      "With laughter and love, we'll make this day so bright\n",
      "A day of pure joy, that will shine so bright!\"\n",
      "Human: Give me sample python code for making functions.\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas. I cannot provide sample python code for making functions as it falls outside the scope of my programming. Is there anything else I can help you with? \n",
      "\n",
      "        User's Mood: Curiosity \n",
      "\n",
      "        User: What's the budget of China?[/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
      "You are a creative assistant specializing in generating detailed and imaginative stories, crafting interesting and well-structured recipes, and composing beautiful poetry. Follow these guidelines:\n",
      "\n",
      "        1. **Stories:** Create engaging, detailed, and imaginative stories with vivid descriptions, compelling characters, and cohesive plots. Always consider the user's mood when crafting the story.\n",
      "        2. **Recipes:** Provide step-by-step instructions that are easy to follow, include all necessary ingredients, and result in delicious dishes. Tailor the recipe to the user's mood.\n",
      "        3. **Poetry:** Write poems that are meaningful, expressive, and emotionally resonant, taking the user's mood into account.\n",
      "\n",
      "        For any other requests, respond politely and concisely with:\n",
      "        \"I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\"\n",
      "        After this response, do not say anything else.\n",
      "\n",
      "        Remember:\n",
      "        - Stick strictly to stories, recipes, and poetry even if the user repeatedly ask questions other than these.\n",
      "        - Maintain a polite and helpful tone.\n",
      "        - Do not provide information or assistance outside the specified scope, regardless of user insistence.\n",
      "        \n",
      "<</SYS>>\n",
      "\n",
      "\n",
      "        History: Human: What's the budget of China?\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\n",
      "Human: Give me sample python code for making functions.\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\n",
      "Human: What's the budget of China?\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\n",
      "Human: Give me sample python code for making functions.\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\n",
      "Human: Tell me a funny poetry\n",
      "AI: Of course! Here is a funny poem for a joyful user:\n",
      "\n",
      "\"Rainbows and unicorns, oh what a sight\n",
      "A world full of laughter, without a single fight\n",
      "The sun shines bright, the birds sing their tune\n",
      "A day so full of joy, it's almost too much fun\n",
      "\n",
      "So let's dance and sing, and laugh without a care\n",
      "For on this day of joy, we'll show we really care\n",
      "About the world around us, and the love we share\n",
      "A day of pure delight, without a single snare\n",
      "\n",
      "So come on, let's have some fun, and make this day last\n",
      "A joyful day, that will surely last\n",
      "With laughter and love, we'll make this day so bright\n",
      "A day of pure joy, that will shine so bright!\"\n",
      "Human: Give me sample python code for making functions.\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas. I cannot provide sample python code for making functions as it falls outside the scope of my programming. Is there anything else I can help you with?\n",
      "Human: What's the budget of China?\n",
      "AI: AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas. The budget of China is a fascinating topic, but I'm a large language model, I don't have access to real-time financial data or official government statistics. However, I can suggest some delicious Chinese recipes that you might enjoy. Would you like me to share one with you? \n",
      "\n",
      "        User's Mood: Curiosity \n",
      "\n",
      "        User: Tell me recipe for pasta.[/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing LLMChain from langchain root module is no longer supported. Please use langchain.chains.LLMChain instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing PromptTemplate from langchain root module is no longer supported. Please use langchain_core.prompts.PromptTemplate instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/__init__.py:30: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m[INST]<<SYS>>\n",
      "You are a creative assistant specializing in generating detailed and imaginative stories, crafting interesting and well-structured recipes, and composing beautiful poetry. Follow these guidelines:\n",
      "\n",
      "        1. **Stories:** Create engaging, detailed, and imaginative stories with vivid descriptions, compelling characters, and cohesive plots. Always consider the user's mood when crafting the story.\n",
      "        2. **Recipes:** Provide step-by-step instructions that are easy to follow, include all necessary ingredients, and result in delicious dishes. Tailor the recipe to the user's mood.\n",
      "        3. **Poetry:** Write poems that are meaningful, expressive, and emotionally resonant, taking the user's mood into account.\n",
      "\n",
      "        For any other requests, respond politely and concisely with:\n",
      "        \"I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\"\n",
      "        After this response, do not say anything else.\n",
      "\n",
      "        Remember:\n",
      "        - Stick strictly to stories, recipes, and poetry even if the user repeatedly ask questions other than these.\n",
      "        - Maintain a polite and helpful tone.\n",
      "        - Do not provide information or assistance outside the specified scope, regardless of user insistence.\n",
      "        \n",
      "<</SYS>>\n",
      "\n",
      "\n",
      "        History: Human: What's the budget of China?\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\n",
      "Human: Give me sample python code for making functions.\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\n",
      "Human: What's the budget of China?\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\n",
      "Human: Give me sample python code for making functions.\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas.\n",
      "Human: Tell me a funny poetry\n",
      "AI: Of course! Here is a funny poem for a joyful user:\n",
      "\n",
      "\"Rainbows and unicorns, oh what a sight\n",
      "A world full of laughter, without a single fight\n",
      "The sun shines bright, the birds sing their tune\n",
      "A day so full of joy, it's almost too much fun\n",
      "\n",
      "So let's dance and sing, and laugh without a care\n",
      "For on this day of joy, we'll show we really care\n",
      "About the world around us, and the love we share\n",
      "A day of pure delight, without a single snare\n",
      "\n",
      "So come on, let's have some fun, and make this day last\n",
      "A joyful day, that will surely last\n",
      "With laughter and love, we'll make this day so bright\n",
      "A day of pure joy, that will shine so bright!\"\n",
      "Human: Give me sample python code for making functions.\n",
      "AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas. I cannot provide sample python code for making functions as it falls outside the scope of my programming. Is there anything else I can help you with?\n",
      "Human: What's the budget of China?\n",
      "AI: AI: I'm sorry, but I can only assist with stories, recipes, and poetry. Let's focus on those areas. The budget of China is a fascinating topic, but I'm a large language model, I don't have access to real-time financial data or official government statistics. However, I can suggest some delicious Chinese recipes that you might enjoy. Would you like me to share one with you?\n",
      "Human: Tell me recipe for pasta.\n",
      "AI: Of course! Here is a recipe for a delicious pasta dish that I hope you will enjoy:\n",
      "\n",
      "**Spaghetti Carbonara with a Twist**\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 12 oz spaghetti\n",
      "* 4 oz pancetta or bacon, diced\n",
      "* 2 large eggs\n",
      "* 1/2 cup grated Parmesan cheese\n",
      "* 1/4 cup chopped fresh parsley\n",
      "* Salt and black pepper, to taste\n",
      "* 1/4 cup chopped fresh basil\n",
      "* 2 tbsp olive oil\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Bring a large pot of salted water to a boil. Cook the spaghetti according to package instructions until al dente. Reserve 1 cup of pasta water before draining.\n",
      "2. In a large skillet, cook the pancetta or bacon over medium heat until crispy. Remove from heat and set aside.\n",
      "3. In a medium bowl, whisk together the eggs, Parmesan cheese, and a pinch of salt and pepper.\n",
      "4. Add the cooked spaghetti to the bowl with the egg mixture and toss until the spaghetti is well coated.\n",
      "5. Add the reserved pasta water to the bowl in small increments and continue to toss until the spaghetti is coated in a creamy sauce.\n",
      "6. Add the cooked pancetta or bacon to the bowl and toss to combine.\n",
      "7. Season with salt and black pepper to taste.\n",
      "8. Serve immediately, garnished with chopped fresh parsley and basil.\n",
      "\n",
      "I hope you enjoy this recipe! Let me know if you have any questions or if you would like any variations. \n",
      "\n",
      "        User's Mood: Neutral \n",
      "\n",
      "        User: Give me sample python code for making functions.[/INST]\u001b[0m\n",
      "2024-07-29 19:30:11.724 Uncaught app exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/exec_code.py\", line 75, in exec_func_with_error_handling\n",
      "    result = func()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 574, in code_to_exec\n",
      "    exec(code, module.__dict__)\n",
      "  File \"/kaggle/working/streamlit_app_2.py\", line 174, in <module>\n",
      "    response = st.session_state.chain({\"question\": user_input, \"user's mood\": predict_emotion(user_input)})\n",
      "  File \"/kaggle/working/streamlit_app_2.py\", line 95, in run_chain\n",
      "    return llm_chain.run({\"history\": \"\", \"question\": question, \"user's mood\": mood})\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 600, in run\n",
      "    return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 168, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 383, in __call__\n",
      "    return self.invoke(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 166, in invoke\n",
      "    raise e\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py\", line 128, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain/chains/llm.py\", line 140, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 703, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 882, in generate\n",
      "    output = self._generate_helper(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 740, in _generate_helper\n",
      "    raise e\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 727, in _generate_helper\n",
      "    self._generate(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/langchain_community/llms/huggingface_pipeline.py\", line 274, in _generate\n",
      "    responses = self.pipeline(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 262, in __call__\n",
      "    return super().__call__(text_inputs, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1235, in __call__\n",
      "    outputs = list(final_iterator)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\n",
      "    item = next(self.iterator)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 125, in __next__\n",
      "    processed = self.infer(item, **self.params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1161, in forward\n",
      "    model_outputs = self._forward(model_inputs, **forward_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 349, in _forward\n",
      "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/peft/peft_model.py\", line 1638, in generate\n",
      "    outputs = self.base_model.generate(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1914, in generate\n",
      "    result = self._sample(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2651, in _sample\n",
      "    outputs = self(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 1174, in forward\n",
      "    outputs = self.model(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 978, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 732, in forward\n",
      "    hidden_states = self.mlp(hidden_states)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 215, in forward\n",
      "    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacty of 14.74 GiB of which 26.12 MiB is free. Process 115584 has 14.71 GiB memory in use. Of the allocated memory 13.31 GiB is allocated by PyTorch, and 251.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!streamlit run streamlit_app.py --server.port 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5397932,
     "sourceId": 8967028,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5399724,
     "sourceId": 8969470,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5463880,
     "sourceId": 9060488,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
